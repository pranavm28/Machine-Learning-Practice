{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc888a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc13f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/seeds.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4405f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[data.columns[0:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012eb18",
   "metadata": {},
   "source": [
    "As you can see, the dataset contains six data points (or features) for each instance (observation) of a seed. So you could interpret these as coordinates that describe each instance's location in six-dimensional space.\n",
    "\n",
    "Six-dimensional space is difficult to visualize in a three-dimensional world, or on a two-dimensional plot. We'll take advantage of a mathematical technique called Principal Component Analysis (PCA) to analyze the relationships between the features and summarize each observation as coordinates for two principal components - in other words, we'll translate the six-dimensional feature values into two-dimensional coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = MinMaxScaler().fit_transform(features[data.columns[0:6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6268e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df888712",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(scaled_features)\n",
    "features_2d = pca.transform(scaled_features)\n",
    "features_2d[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3770f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_2d[:,0],features_2d[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i)\n",
    "    # Fit the data points\n",
    "    kmeans.fit(features.values)\n",
    "    # Get the WCSS (inertia) value\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('WCSS by Clusters')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d335a84",
   "metadata": {},
   "source": [
    "Hopefully you can see at least two, arguably three, reasonably distinct groups of data points. This shows one of the fundamental problems with clustering - without known class labels, how do you know how many clusters to separate your data into?\n",
    "\n",
    "One way we can try to find out is to use a data sample to create a series of clustering models with an incrementing number of clusters, and measure how tightly the data points are grouped within each cluster. A metric often used to measure this tightness is the within cluster sum of squares (WCSS), with lower values meaning that the data points are closer. You can then plot the WCSS for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db5c61",
   "metadata": {},
   "source": [
    "The plot shows a large reduction in WCSS (so greater tightness) as the number of clusters increases from one to two, and a further noticable reduction from two to three clusters. After that, the reduction is less pronounced, resulting in an \"elbow\" in the chart at around three clusters. This is a good indication that there are two to three reasonably well separated clusters of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350ee9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
